{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "99ed89f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys, PandasTools\n",
    "import pubchempy as pcp\n",
    "import glob\n",
    "import pickle\n",
    "import urllib.request, json\n",
    "import re\n",
    "from sklearn.decomposition import FastICA, PCA, IncrementalPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e3884",
   "metadata": {},
   "source": [
    "# PubChem\n",
    "\n",
    "Note that all steps involving the PubChem data are rather lengthy as they involve huge amounts of data. Hence, we refrain from providing the intermediate datasets and refer the user to the original website to download the data and pre-process it according to our script.\n",
    "\n",
    "PubChem Download: https://ftp.ncbi.nlm.nih.gov/pubchem/Compound/CURRENT-Full/SDF/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b7803",
   "metadata": {},
   "source": [
    "#### Extract unique SMILES from PubChem\n",
    "\n",
    "1. First, we downloaded all compounds in sdf format in batches from PubChem and unpacked each of the batches.\n",
    "2. Starting from the completed unpacking step, we restrict each file to only the canonical SMILES to reduce the workload of subsequent steps.\n",
    "3. We keep only the unique SMILES within each batch, and then iteratively concatenate all batches while only keeping unique SMILES.\n",
    "4. Finally, we store the list of all unique SMILES as unique_smiles.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f605441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_TO_UNZIPPED_SDF = ''  # set path to downloaded and unpacked PubChem sdf files\n",
    "# PATH_TO_EXTRACTED_SMILES = ''  # set path to folder where the extracted smiles will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ac3a18a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # convert sdf to SMILES files (not unique!)\n",
    "# for f in [glob.glob(PATH_TO_UNZIPPED_SDF)]:\n",
    "#     print(f)\n",
    "#     try:\n",
    "#         df = PandasTools.LoadSDF(f, molColName=None)\n",
    "#         df.to_csv(PATH_TO_EXTRACTED_SMILES+'\\\\'+f.split('.')[0]+'.txt', columns=['PUBCHEM_OPENEYE_CAN_SMILES'], header=False, index=False)\n",
    "#     except:\n",
    "#         print('ERROR in', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46819a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge files; keep only unique smiles\n",
    "# files_smiles = glob.glob(PATH_TO_EXTRACTED_SMILES+'\\\\*')\n",
    "# df = pd.read_csv(files_smiles[0], header=None)\n",
    "# df = df.drop_duplicates(subset=0, keep='first')\n",
    "# df = df.set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47e8c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in files_smiles[1:]:\n",
    "#     try:\n",
    "#         df2 = pd.read_csv(f, header=None)\n",
    "#         df2 = df2.drop_duplicates(subset=0, keep='first')\n",
    "#         df2 = df2.set_index(0)\n",
    "#         df = df.combine_first(df2)\n",
    "#     except:\n",
    "#         print('Stopped at', f)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42e2f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(PATH_TO_EXTRACTED_SMILES+'\\\\unique_smiles.txt', header=False, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97fe1b8",
   "metadata": {},
   "source": [
    "#### Calculate incremental PCA\n",
    "\n",
    "To locate other datasets in the PubChem PCA space, we need to train PCA for PubChem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cc796cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_PCA_MODELS = 'Data\\\\PCA'  # set path to folder where all intermediate results for PCA calculations should be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bbde4fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # a balanced batch-size is 5*n_features, that's just below 1000 here.\n",
    "# chunksize = 10**3\n",
    "# done = 0\n",
    "# pca_pubchem = IncrementalPCA(n_components=10)\n",
    "# for chunk in pd.read_csv(PATH_TO_EXTRACTED_SMILES+'\\\\unique_smiles.txt', header=None, chunksize=chunksize):\n",
    "#     res, smiles = [], []\n",
    "#     for i in range(len(chunk)):\n",
    "#         try:\n",
    "#             mol = Chem.MolFromSmiles(chunk[0][done+i])\n",
    "#             maccs = MACCSkeys.GenMACCSKeys(mol)\n",
    "#             res.append(maccs)\n",
    "#         except: \n",
    "#             continue\n",
    "#     pca_pubchem.partial_fit(res)\n",
    "#     done += chunksize\n",
    "#     if (done % (100*chunksize)) == 0: # write only after each 10th iteration\n",
    "#         pickle.dump(pca_pubchem, open(PATH_TO_PCA_MODELS+'\\\\PCA_PubChem_model.pickle', 'wb'))\n",
    "#         print('%.2f percent done' % (done / 634417.97), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6753a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pubchem = pickle.load(open(PATH_TO_PCA_MODELS+'\\\\PCA_PubChem_model.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0079c05",
   "metadata": {},
   "source": [
    "#### Calculate histogram to estimate the PubChem density in PCA space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bee1516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_file = PATH_TO_PCA_MODELS+'\\\\PCA_PubChem.txt'\n",
    "# open(out_file, mode='w').close()\n",
    "# #num_lines = sum(1 for line in open(PATH_TO_EXTRACTED_SMILES+'\\\\unique_smiles.txt')) #63441797\n",
    "\n",
    "# chunksize = 10**5\n",
    "# done = 0\n",
    "# for chunk in pd.read_csv(PATH_TO_EXTRACTED_SMILES+'\\\\unique_smiles.txt', header=None, chunksize=chunksize):\n",
    "#     res, smiles = [], []\n",
    "#     for i in range(len(chunk)):\n",
    "#         try:\n",
    "#             mol = Chem.MolFromSmiles(chunk[0][done+i])\n",
    "#             maccs = MACCSkeys.GenMACCSKeys(mol)\n",
    "#             smiles.append(chunk[0][done+i])\n",
    "#             res.append(maccs)\n",
    "#         except: \n",
    "#             continue\n",
    "#     pd.DataFrame(pca_pubchem.transform(res)).assign(smiles = smiles).to_csv(out_file, header=False, index=False, mode='a')\n",
    "#     done += chunksize\n",
    "#     print('%.2f percent done' % (done / 634417.97), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa9d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find min/max in the PCA space \n",
    "# low, high = np.array([np.Inf]*10), np.array([-np.Inf]*10)\n",
    "# for chunk in pd.read_csv(PATH_TO_PCA_MODELS+'\\\\PCA_PubChem.txt', chunksize=10**5, header=None):\n",
    "#     low = np.minimum(chunk[range(10)].min(axis=0), low)\n",
    "#     high = np.maximum(chunk[range(10)].max(axis=0), high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fcb077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write to file so you won't have to search for that again\n",
    "# df = pd.DataFrame(np.vstack((low, high)))\n",
    "# df['index'] = ['min', 'max']\n",
    "# df = df.set_index('index')\n",
    "# df.to_csv(PATH_TO_PCA_MODELS+'PCA_PubChem_minmax.txt', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee596fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read in min/max for all PCs\n",
    "df = pd.read_csv(PATH_TO_PCA_MODELS+'\\\\PCA_PubChem_minmax.txt', header=0, index_col='index')\n",
    "low_pubchem, high_pubchem = df.loc['min'].to_numpy(), df.loc['max'].to_numpy()\n",
    "range_hist_pubchem = df[['0', '1']].to_numpy().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9b98c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build chunk-wise histogram\n",
    "# bins = 30\n",
    "# H, xedges, yedges = np.histogram2d([], [], bins=bins, range=range_hist_pubchem) # empty histogram\n",
    "# for chunk in pd.read_csv(PATH_TO_PCA_MODELS+'\\\\PCA_PubChem.txt', chunksize=10**5, header=None):\n",
    "#     H_chunk, _, _ = np.histogram2d(chunk[0], chunk[1], bins=bins, range=range_hist_pubchem)\n",
    "#     H += H_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71be2157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store histogram in file for later use\n",
    "# hist_dict = {'H': H, 'xedges': xedges, 'yedges': yedges}\n",
    "# np.save(PATH_TO_PCA_MODELS+'\\\\PCA_PubChem_hist_30.npy', hist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4bfae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read histogram from file\n",
    "hist_dict = np.load(PATH_TO_PCA_MODELS+'\\\\PCA_PubChem_hist_30.npy', allow_pickle=True)\n",
    "H, xedges, yedges = hist_dict[()]['H'], hist_dict[()]['xedges'], hist_dict[()]['yedges']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0cbb6",
   "metadata": {},
   "source": [
    "**Note**: We similarly obtained the PubChem histogram and min/max for the PCA spaces resulting from the SOIL and BBD datasets (see below).\n",
    "\n",
    "The results are stored in the same file format with prefix PCA_SOIL or PCA_BBD instead of PCA_PubChem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60085162",
   "metadata": {},
   "source": [
    "## Agrochemical Subset\n",
    "\n",
    "Download from https://pubchem.ncbi.nlm.nih.gov/#input_type=list&query=VRLzGHyuGRIuPJElE13YCQ42k1bcapT67t-PtvXOnbf116E&collection=compound&alias=PubChem%3A%20PubChem%20Compound%20TOC%3A%20Agrochemical%20Information\n",
    "\n",
    "The pre-processing of this subset of PubChem is equivalent to that of the PubChem data above. Additionally, we can extract information on the type of the compounds and what they are used for. Those free texts are processed similar to those in SOIL/BBD (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "efc29972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PATH_TO_AGRO_SDF = ''  # set path to downloaded and unpacked Agrochemical sdf files\n",
    "# DF_AGRO = PandasTools.LoadSDF(PATH_TO_AGRO_SDF, molColName=None)\n",
    "# DF_AGRO = DF_AGRO.rename(columns={'PUBCHEM_OPENEYE_CAN_SMILES': 'smiles'})\n",
    "# DF_AGRO = DF_AGRO.drop_duplicates(keep='first', subset='smiles')\n",
    "# mol = [Chem.MolFromSmiles(s) for s in DF_AGRO.smiles]\n",
    "# DF_AGRO.drop([i for i, j in enumerate(mol) if j is None], inplace=True)\n",
    "# DF_AGRO = DF_AGRO.reset_index(drop=True)\n",
    "# agro_maccs = [np.array(MACCSkeys.GenMACCSKeys(m)) for m in [m for m in mol if m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1a680037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_agro_categories(cid):\n",
    "#     rest_url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/\"+str(cid)+\"/JSON\"\n",
    "#     with urllib.request.urlopen(rest_url) as url:\n",
    "#         #data = json.loads(url.read().decode())\n",
    "#         data = json.loads(url.read().decode(\"utf-8\", errors='ignore'))\n",
    "#     dicts = data.get('Record').get('Section')\n",
    "#     dicts_agro = next((item for item in dicts if item[\"TOCHeading\"] == \"Agrochemical Information\"), None)\n",
    "#     if dicts_agro is None: return None \n",
    "#     else: dicts_agro = dicts_agro.get('Section')\n",
    "#     list_agro = next((item for item in dicts_agro if item[\"TOCHeading\"] == \"Agrochemical Category\"), None)\n",
    "#     if list_agro is None: return None \n",
    "#     else: list_agro = list_agro.get('Information', None)\n",
    "#     if list_agro is None: return None\n",
    "#     categories = [list_agro[i].get('Value').get('StringWithMarkup')[0].get('String') for i in range(len(list_agro))]\n",
    "#     return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "620a9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_AGRO['agro_categories'] = [get_agro_categories(cid) for cid in DF_AGRO.PUBCHEM_COMPOUND_CID]\n",
    "# cats = DF_AGRO.agro_categories.tolist()\n",
    "# for i in DF_AGRO.PUBCHEM_COMPOUND_CID:\n",
    "#     cats[DF_AGRO.index[DF_AGRO['PUBCHEM_COMPOUND_CID'] == str(i)].tolist()[0]] = get_agro_categories(i)\n",
    "# cats = [[] if c is None else c for c in cats]\n",
    "# DF_AGRO['agro_categories'] = cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3fedcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unravel the free text\n",
    "\n",
    "# flattened = [j for sub in cats for j in sub]\n",
    "# flattened = np.unique(flattened)\n",
    "# unflat = [f.split(',') for f in flattened]\n",
    "# flattened = [j for sub in unflat for j in sub]\n",
    "# unflat = [f.split('->') for f in flattened]\n",
    "# flattened = [j for sub in unflat for j in sub]\n",
    "# flattened = [f.strip().lower().removesuffix('s') for f in flattened]\n",
    "# flattened = np.unique(flattened) # categories\n",
    "\n",
    "# res = np.empty((len(DF_AGRO), len(flattened)))\n",
    "# for i, cat in enumerate(flattened):\n",
    "#     res[:,i] = [str(cat) in str(cats[i]).lower() for i in range(len(cats))]\n",
    "\n",
    "# df_props = pd.DataFrame(res, columns=flattened, dtype=int)\n",
    "# df_props['algicide'] = df_props.algicide + df_props.algaecide\n",
    "# df_props['repellent'] = df_props.repellent + df_props.repellant\n",
    "# df_props['plant growth regulator'] = df_props['growth reg.'] + df_props['plant growth regulator']\n",
    "# df_props.drop(['algaecide', 'other substance', 'other treatment', 'repellant', 'special use', 'growth reg.'], axis=1, inplace=True)\n",
    "# df_agro_prop = DF_AGRO.join(df_props, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add information which dataset is contained in AGRO (read SOIL/BBD first!)\n",
    "\n",
    "# mol_agro = [Chem.MolFromSmiles(s) for s in df_agro_prop.smiles]\n",
    "# mol_bbd = [Chem.MolFromSmiles(s) for s in DF_BBD.SMILES]\n",
    "# mol_soil = [Chem.MolFromSmiles(s) for s in DF_SOIL.SMILES]\n",
    "\n",
    "# smiles_agro = [(None if m is None else Chem.MolToSmiles(m)) for m in mol_agro]\n",
    "# smiles_bbd = [(None if m is None else Chem.MolToSmiles(m)) for m in mol_bbd]\n",
    "# smiles_soil = [(None if m is None else Chem.MolToSmiles(m)) for m in mol_soil]\n",
    "\n",
    "# agro_in_bbd = [smiles_agro[i] in smiles_bbd for i in range(len(smiles_agro))]\n",
    "# agro_in_soil = [smiles_agro[i] in smiles_soil for i in range(len(smiles_agro))]\n",
    "# df_agro_prop['isInSOIL'] = agro_in_soil\n",
    "# df_agro_prop['isInBBD'] = agro_in_bbd\n",
    "\n",
    "# df_agro_prop.to_csv('Data\\\\AGRO_all.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4fd0284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_AGRO = pd.read_csv('Data\\\\AGRO_all.csv', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122fe048",
   "metadata": {},
   "source": [
    "# SOIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056be5a",
   "metadata": {},
   "source": [
    "#### Read in Dataset\n",
    "\n",
    "1. Read all SMILES from the enviPath platform.\n",
    "2. Obtain the corresponding MACCS keys using RDKit (these will be used as features).\n",
    "3. Drop those SMILES that cannot be processed using RDKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9156313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from enviPath_python.enviPath import *\n",
    "# from enviPath_python.objects import *\n",
    "\n",
    "# # extract all smiles from enviPath\n",
    "# EAWAG_SOIL = 'https://envipath.org/package/5882df9c-dae1-4d80-a40e-db4724271456'\n",
    "# soil_smiles = get_smiles_from_envipath(EAWAG_SOIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0643915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_maccs(compound):\n",
    "#     try:\n",
    "#         mol = Chem.MolFromSmiles(compound)\n",
    "#         return MACCSkeys.GenMACCSKeys(mol)\n",
    "#     except:\n",
    "#         return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93bade36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_smiles_from_envipath(package):\n",
    "#     eP = enviPath('https://envipath.org/')\n",
    "#     p = eP.get_package(package)\n",
    "#     compounds = p.get_compounds()\n",
    "#     smiles = []\n",
    "#     for i, c in enumerate(compounds):\n",
    "#         smiles.append(c.get_smiles())\n",
    "#         print(i/len(compounds)*100, '% done', end='\\r')\n",
    "#     return smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28b37ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_SOIL = pd.DataFrame(soil_smiles, columns=['SMILES'])\n",
    "# DF_SOIL['MACCS'] = [get_maccs(c) for c in DF_SOIL.SMILES]\n",
    "# DF_SOIL.dropna(subset=['MACCS'], inplace=True)\n",
    "# soil_maccs = np.stack(DF_SOIL.MACCS.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc99a52",
   "metadata": {},
   "source": [
    "#### Join Discovery Years for Root Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96375f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obtain list containing the years for root compounds\n",
    "\n",
    "# eP = enviPath('https://envipath.org')\n",
    "# soil_package = Package(eP.requester, id=EAWAG_SOIL)\n",
    "\n",
    "# # get pathways\n",
    "# pathways = soil_package.get_pathways()\n",
    "\n",
    "# # open output file & set header\n",
    "# outfile = open('SOIL_root_dates.tsv', 'w')\n",
    "# outfile.write('root_node_SMILES\\tdate\\tpathway_link\\n')\n",
    "\n",
    "# all_soil_scenarios = soil_package.get_scenarios()\n",
    "# def list_to_dict(all_scenarios):\n",
    "#     scen_dict = {}\n",
    "#     for scen in all_scenarios:\n",
    "#         scen_dict[scen.get_name()] = scen.get_id()\n",
    "#     return scen_dict\n",
    "\n",
    "# def get_main_scen_name(ref_scen_name):\n",
    "#     return ref_scen_name.split(' (Related Scenario) - ')[0]\n",
    "\n",
    "# scen_dict = list_to_dict(all_soil_scenarios)\n",
    "\n",
    "# # iterate through pathways in soil\n",
    "# for pathway in pathways:\n",
    "#     for node in pathway._get('nodes'):\n",
    "#         if node.get('depth') == 0: # only consider root nodes (at depth == 0)\n",
    "#             smiles = node.get('smiles') # fetch the smiles of the node\n",
    "#             scenarios = node.get('proposed') # fetch a list of scenario links\n",
    "#             all_dates = set([]) # container to keep unique dates\n",
    "#             checked_main_scenarios = []\n",
    "#             for scenario in scenarios:\n",
    "#                 this_scenario = Scenario(eP.requester, id=scenario.get('scenarioId')) # load full scenario\n",
    "#                 all_dates.add(this_scenario._get('date')) # fetch the date\n",
    "#                 # if it's a related scenario, also check for dates in the main scenario\n",
    "#                 if '(Related Scenario)' in this_scenario.get_name():\n",
    "#                     main_scenario_name = get_main_scen_name(this_scenario.get_name())\n",
    "#                     try:\n",
    "#                         main_scenario_id = scen_dict[main_scenario_name]\n",
    "#                     except KeyError:\n",
    "#                         print(\"Warning: no id found for main scenario name {} \"\n",
    "#                               \"derived from referring scenario id {}\".format(main_scenario_name, this_scenario.get_id()))\n",
    "#                     else:\n",
    "#                         if main_scenario_id not in checked_main_scenarios: # check if already analyzed\n",
    "#                             main_scenario = Scenario(eP.requester, id=main_scenario_id)\n",
    "#                             all_dates.add(main_scenario._get('date'))\n",
    "#                             checked_main_scenarios.append(main_scenario_id)\n",
    "#             outfile.write('{}\\t{}\\t{}\\n'.format(smiles, ';'.join(all_dates), pathway.get_id())) # write to output file\n",
    "#             continue # no need to search further since we found the root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "beb73853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract years from dates\n",
    "# df_tmp = pd.read_csv('Data/SOIL_root_dates.tsv', sep='\\t', header=0)\n",
    "# def getdate(s):\n",
    "#     if s is np.nan: return 0\n",
    "#     a = re.findall(r'.*([1-3][0-9]{3})', s)\n",
    "#     if len(a) == 0: return 0\n",
    "#     return int(a[0])\n",
    "# df_tmp['year'] = [getdate(df_tmp.date[i]) for i in range(len(df_tmp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "84074cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_SOIL = DF_SOIL.merge(df_tmp[['root_node_SMILES', 'year']], left_on='SMILES', right_on='root_node_SMILES', how='left')\n",
    "# DF_SOIL = DF_SOIL.drop(['root_node_SMILES'], axis=1)\n",
    "# DF_SOIL['isRootCompound'] = DF_SOIL['year']>=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1025dd",
   "metadata": {},
   "source": [
    "#### Find the compounds in PubChem (if possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "66cf2517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cids = np.zeros(len(DF_SOIL))\n",
    "# for i, smile in enumerate(DF_SOIL.SMILES):\n",
    "#     try:\n",
    "#         cids[i] = pcp.get_compounds(smile, 'smiles')[0].cid\n",
    "#     except:\n",
    "#         continue\n",
    "#     print('Completed', i/len(DF_SOIL)*100, '%', end='\\r')\n",
    "# DF_SOIL['cid'] = cids\n",
    "# np.save('Data\\\\SOIL_cids.npy', cids)\n",
    "# DF_SOIL['cid'] = np.load('Data\\\\SOIL_cids.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31664c6",
   "metadata": {},
   "source": [
    "#### Query Use Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6a9a79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_use_categories(cid):\n",
    "#     rest_url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/\"+str(cid)+\"/JSON\"\n",
    "#     with urllib.request.urlopen(rest_url) as url:\n",
    "#         #data = json.loads(url.read().decode())\n",
    "#         data = json.loads(url.read().decode(\"utf-8\", errors='ignore'))\n",
    "#     dicts = data.get('Record').get('Section')\n",
    "#     dicts_agro = next((item for item in dicts if item[\"TOCHeading\"] == \"Use and Manufacturing\"), None)\n",
    "#     if dicts_agro is None: return None \n",
    "#     else: dicts_agro = dicts_agro.get('Section')\n",
    "#     list_agro = next((item for item in dicts_agro if item[\"TOCHeading\"] == \"Uses\"), None)\n",
    "#     if list_agro is None: return None \n",
    "#     else: list_agro = list_agro.get('Section')[0].get('Information')\n",
    "#     if list_agro is None: return None\n",
    "#     categories = [list_agro[i].get('Value').get('StringWithMarkup')[0].get('String') for i in range(len(list_agro))]\n",
    "#     return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "305c699e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # query Use Categories from PubChem\n",
    "# use_cats = []\n",
    "# for i, cid in enumerate(cids.astype(np.int64)):\n",
    "#     try:\n",
    "#         use_cats.append(get_use_categories(cid))\n",
    "#     except:\n",
    "#         use_cats.append([])\n",
    "#         continue\n",
    "#     print('Completed', i/len(cids)*100, '%', end='\\r')\n",
    "# DF_SOIL['use_cats'] = use_cats\n",
    "# np.save('Data\\\\SOIL_use_cats.npy', use_cats)\n",
    "# DF_SOIL['use_cats'] = np.load('Data\\\\SOIL_use_cats.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5475912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pre-process free-text field\n",
    "# use_cats = [[] if c is None else c for c in DF_SOIL['use_cats'].values]\n",
    "# use_cats_concat = [','.join(s) for s in use_cats]\n",
    "# use_cats_concat = [s.lower() for s in use_cats_concat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f41bc204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # search for specific terms\n",
    "# USE_CATS = ['acaricide', 'attractant', 'biocide', 'fungicide', 'herbicide', 'insecticide', \n",
    "#             'pesticide', 'growth reg', 'transformation product', 'drug', 'food', 'health hazard', 'fire hazard']\n",
    "\n",
    "# soil_categories = np.empty((len(DF_SOIL), len(USE_CATS)))\n",
    "# for j, c in enumerate(USE_CATS):\n",
    "#     for i in range(len(DF_SOIL)):\n",
    "#         soil_categories[i, j] = c in use_cats_concat[i]\n",
    "        \n",
    "# DF_SOIL[USE_CATS] = soil_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731814f3",
   "metadata": {},
   "source": [
    "#### Agrochemical subset of PubChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "051c182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get canonical smiles representation for more promising comparison\n",
    "# mol_agro = [Chem.MolFromSmiles(s) for s in DF_AGRO.smiles]\n",
    "# mol_soil = [Chem.MolFromSmiles(s) for s in DF_SOIL.SMILES]\n",
    "# smiles_agro = [(None if m is None else Chem.MolToSmiles(m)) for m in mol_agro]\n",
    "# smiles_soil = [(None if m is None else Chem.MolToSmiles(m)) for m in mol_soil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "88cdd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a flag indicating if the compound is contained in the agrochemical subset of PubChem\n",
    "# DF_SOIL['SMILES_canon'] = smiles_soil\n",
    "# DF_SOIL['isInAgro'] = [i in smiles_agro for i in smiles_soil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bdd3efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_SOIL.to_csv('Data\\\\SOIL_all.csv', header=True)\n",
    "DF_SOIL = pd.read_csv('Data/SOIL_all.csv', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03883d6",
   "metadata": {},
   "source": [
    "# BBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017ef64",
   "metadata": {},
   "source": [
    "#### Read in Dataset\n",
    "\n",
    "1. Read all SMILES from the enviPath platform.\n",
    "2. Obtain the corresponding MACCS keys using RDKit (these will be used as features).\n",
    "3. Drop those SMILES that cannot be processed using RDKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0f25a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract all smiles from enviPath\n",
    "# EAWAG_BBD = 'https://envipath.org/package/32de3cf4-e3e6-4168-956e-32fa5ddb0ce1'\n",
    "# bbd_smiles = get_smiles_from_envipath(EAWAG_BBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9a264e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_BBD = pd.DataFrame(bbd_smiles, columns=['SMILES'])\n",
    "# DF_BBD['MACCS'] = [get_maccs(c) for c in DF_BBD.SMILES]\n",
    "# DF_BBD.dropna(subset=['MACCS'], inplace=True)\n",
    "# bbd_maccs = np.stack(DF_BBD.MACCS.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a99a4",
   "metadata": {},
   "source": [
    "#### Join Discovery Years for Root Compounds\n",
    "\n",
    "For BBD, we use the last date of a professional update (via webserver). This date most likely coincides with the date when the compound was added since \"normal\" users access the compounds mainly via SQL and trigger different events. For the sake of reproducibility, we provide those dates in the file \"BBD_dates.tsv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "453fd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp = pd.read_csv('Data/BBD_dates.tsv', sep='\\t', header=0)\n",
    "# df_tmp['year'] = [getdate(df_tmp.date[i]) for i in range(len(df_tmp))]\n",
    "\n",
    "# DF_BBD = DF_BBD.merge(df_tmp[['smiles', 'year']], left_on='SMILES', right_on='smiles', how='left')\n",
    "# DF_BBD = DF_BBD.drop(['smiles'], axis=1)\n",
    "# DF_BBD['isRootCompound'] = DF_BBD['year']>=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8211fa49",
   "metadata": {},
   "source": [
    "#### Find the compounds in PubChem (if possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "472827eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cids = np.zeros(len(DF_BBD))\n",
    "# for i, smile in enumerate(DF_BBD.SMILES):\n",
    "#     try:\n",
    "#         cids[i] = pcp.get_compounds(smile, 'smiles')[0].cid\n",
    "#     except:\n",
    "#         continue\n",
    "#     print('Completed', i/len(DF_BBD)*100, '%', end='\\r')\n",
    "# DF_BBD['cid'] = cids\n",
    "# np.save('Data\\\\BBD_cids.npy', cids)\n",
    "# DF_BBD['cid'] = np.load('Data\\\\BBD_cids.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96968d",
   "metadata": {},
   "source": [
    "#### Query Use Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "fee4cbb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # query Use Categories from PubChem\n",
    "# use_cats = []\n",
    "# for i, cid in enumerate(cids.astype(np.int64)):\n",
    "#     try:\n",
    "#         use_cats.append(get_use_categories(cid))\n",
    "#     except:\n",
    "#         use_cats.append([])\n",
    "#         continue\n",
    "#     print('Completed', i/len(cids)*100, '%', end='\\r')\n",
    "# DF_BBD['use_cats'] = use_cats\n",
    "# np.save('Data\\\\BBD_use_cats.npy', use_cats)\n",
    "# DF_BBD['use_cats'] = np.load('Data\\\\BBD_use_cats.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8fdd9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pre-process free-text field\n",
    "# use_cats = [[] if c is None else c for c in DF_BBD['use_cats'].values]\n",
    "# use_cats_concat = [','.join(s) for s in use_cats]\n",
    "# use_cats_concat = [s.lower() for s in use_cats_concat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b0a88799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # search for specific terms\n",
    "# bbd_categories = np.empty((len(DF_BBD), len(USE_CATS)))\n",
    "# for j, c in enumerate(USE_CATS):\n",
    "#     for i in range(len(DF_BBD)):\n",
    "#         bbd_categories[i, j] = c in use_cats_concat[i]\n",
    "        \n",
    "# DF_BBD[USE_CATS] = bbd_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a327eb43",
   "metadata": {},
   "source": [
    "#### Agrochemical subset of PubChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "190235c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get canonical smiles representation for more promising comparison\n",
    "# mol_bbd = [Chem.MolFromSmiles(s) for s in DF_BBD.SMILES]\n",
    "# smiles_bbd = [(None if m is None else Chem.MolToSmiles(m)) for m in mol_bbd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b0624512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a flag indicating if the compound is contained in the agrochemical subset of PubChem\n",
    "# DF_BBD['SMILES_canon'] = smiles_bbd\n",
    "# DF_BBD['isInAgro'] = [i in smiles_agro for i in smiles_bbd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "80cb49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_BBD.to_csv('Data\\\\BBD_all.csv', header=True)\n",
    "DF_BBD = pd.read_csv('Data/BBD_all.csv', header=0, index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
